{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import hdf5_getters\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn import datasets\n",
    "import random\n",
    "random.seed(3222)\n",
    "np.random.seed(3222)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100\n"
     ]
    }
   ],
   "source": [
    "def get_data(basedir, function, upto=10000, ext='.h5'):\n",
    "    data = []\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(basedir):\n",
    "        files = glob.glob(os.path.join(root,'*'+ext))\n",
    "        for f in files:\n",
    "            if count == upto:\n",
    "                return data\n",
    "            if count%100 == 0:\n",
    "                print count,\n",
    "            h5 = hdf5_getters.open_h5_file_read(f)\n",
    "            data.append(function(h5))\n",
    "            h5.close()\n",
    "            count += 1\n",
    "    return data\n",
    "\n",
    "# save as pickle to save time in loading\n",
    "segments_pitches_1000 = get_data('../MillionSongSubset/data', hdf5_getters.get_segments_pitches, upto=200)\n",
    "pickle.dump(segments_pitches_1000, open(\"../MillionSongSubset/pitches1000\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## squeeze vocab data into one array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocab set\n",
      "91912\n",
      "(91912, 12)\n"
     ]
    }
   ],
   "source": [
    "# use first 100 songs to create vocabulary\n",
    "vocab_size = 100\n",
    "def build_set(data):\n",
    "    num_of_items = 0\n",
    "    for i in range(len(data)):\n",
    "        num_of_items += data[i].shape[0]\n",
    "    print num_of_items\n",
    "    results = np.ndarray((num_of_items, data[0].shape[1]))\n",
    "    count = 0\n",
    "    for i in range(len(data)):\n",
    "        for j in range(data[i].shape[0]):\n",
    "            results[count] = data[i][j]\n",
    "            count += 1\n",
    "    return results\n",
    "print \"building vocab set\"\n",
    "vocab_pitches = build_set(segments_pitches_1000[:100])\n",
    "print vocab_pitches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_points_to_clusters(centroids, points, k):\n",
    "    # 1 list for each centroid (will contain indices of points)\n",
    "    clusters = [[] for i in range(k)]\n",
    "    for i in range(points.shape[0]):\n",
    "        # find nearest centroid to this point\n",
    "        best_centroid = 0\n",
    "        best_distance = euclidean(centroids[best_centroid], points[i])\n",
    "        for c in range(1, k):\n",
    "            distance = euclidean(centroids[c], points[i])\n",
    "            if distance < best_distance:\n",
    "                best_distance = distance\n",
    "                best_centroid = c\n",
    "        clusters[best_centroid].append(i)\n",
    "    return clusters\n",
    "\n",
    "def update_centroids(centroids, clusters, points, k):\n",
    "    for ci in range(k):\n",
    "        if clusters[ci]:\n",
    "            sum_points = np.zeros(points.shape[1])\n",
    "            for point in clusters[ci]:\n",
    "                sum_points += points[point]\n",
    "                #print sum_points\n",
    "            centroids[ci] = sum_points/len(clusters[ci])\n",
    "\n",
    "def mykmeans(points, k, max_iter=50, centroids=None):\n",
    "    n_samples, n_features = points.shape\n",
    "    if not centroids:\n",
    "        centroids = [points[x] for x in random.sample(range(n_samples), k)]\n",
    "        # seeds = random_state.permutation(n_samples)[:k]\n",
    "    clusters = None\n",
    "    for p in range(max_iter):\n",
    "        print p,\n",
    "        clusters = assign_points_to_clusters(centroids, points, k)\n",
    "        update_centroids(centroids, clusters, points, k)\n",
    "    return centroids, clusters\n",
    "        \n",
    "def mykmeansplusplus(points, k, max_iter=50):\n",
    "    n_samples, n_features = points.shape\n",
    "    centroids = []\n",
    "    centroids.append(np.random.randint(0, n_samples))\n",
    "    for i in range(k-1):\n",
    "        weightProb = []\n",
    "        leftPoints = [p for p in range(n_samples) if p not in centroids]\n",
    "        for p in leftPoints:\n",
    "            best_D = euclidean(points[p], points[centroids[0]])\n",
    "            best_p = centroids[0]\n",
    "            for c in range(1, len(centroids)):\n",
    "                D = euclidean(points[p], points[centroids[c]])\n",
    "                if D < best_D:\n",
    "                    best_D = D\n",
    "            weightProb.append(best_D)\n",
    "        cum = sum(weightProb)\n",
    "        weightProb = [p/cum for p in weightProb]\n",
    "        centroids.append(np.random.choice(leftPoints, p=weightProb))\n",
    "    centroids = [points[c] for c in centroids]\n",
    "    return mykmeans(points, k, max_iter, centroids)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learnvocabulary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "def learnvocabulary(vocab_data, num_clusters, max_iter=50):\n",
    "    return mykmeansplusplus(vocab_data, num_clusters, max_iter)\n",
    "centroids, clusters = learnvocabulary(vocab_pitches, k, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getbof function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.21734587  0.32810867  0.09195402  0.36259143]\n"
     ]
    }
   ],
   "source": [
    "def getbof(centroids, signal):\n",
    "    bof_tf = np.zeros(len(centroids))\n",
    "    predicted_clusters = assign_points_to_clusters(centroids, signal, len(centroids))\n",
    "    for i in range(len(centroids)):\n",
    "        bof_tf[i] = len(predicted_clusters[i])\n",
    "    # make it sum to 1\n",
    "    return bof_tf/sum(bof_tf)\n",
    "print getbof(centroids, segments_pitches_1000[199])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
